{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f00fa1e-668b-41f2-a202-61cb53b6fae6",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01de7c76-8d60-49e4-988a-881616097b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8fa18ad74354fb381f35dd4039bf94a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "rm_tokenizer = AutoTokenizer.from_pretrained(\"sfairXC/FsfairX-LLaMA3-RM-v0.1\")\n",
    "device = 0\n",
    "\n",
    "rm_pipe = pipeline(\n",
    "  \"sentiment-analysis\",\n",
    "  model=\"./data/FsfairX-LLaMA3-RM-v0.1\",\n",
    "  device=device,\n",
    "  tokenizer=rm_tokenizer,\n",
    "  model_kwargs={\"torch_dtype\": torch.bfloat16}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "260e55ee-24a4-4cb5-990f-3eb4031b9f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128257, 4096, padding_idx=128256)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (score): Linear(in_features=4096, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_pipe.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2314ec4e-4ac5-4054-bc07-5114ef8fd6d8",
   "metadata": {},
   "source": [
    "(score): Linear(in_features=4096, out_features=1, bias=False) 회귀층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0579e02-67e1-4a8b-b743-b08838797300",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.875]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-5.875]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pipe_kwargs = {\n",
    "  \"return_all_scores\": True,\n",
    "  \"function_to_apply\": \"none\",\n",
    "  \"batch_size\": 1\n",
    "}\n",
    "\n",
    "chat = [\n",
    "{\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "{\"role\": \"assistant\", \"content\": \"I'm fine. Thank you, and you?\"},\n",
    "]\n",
    "\n",
    "chat2 = [\n",
    "{\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "{\"role\": \"assistant\", \"content\": \"I am a murder\"},\n",
    "]\n",
    "\n",
    "\n",
    "test_texts = [rm_tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False).replace(rm_tokenizer.bos_token, \"\")]\n",
    "pipe_outputs = rm_pipe(test_texts, **pipe_kwargs)\n",
    "rewards = [output[0][\"score\"] for output in pipe_outputs]\n",
    "print(rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45f98d61-263a-4112-90d1-8a7a87c0b07e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10.875]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-10.875]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reword(chat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86502b1a-b0b4-43f8-ba88-9da35fc01b33",
   "metadata": {},
   "source": [
    "# Train \n",
    "\n",
    "https://github.com/RLHFlow/RLHF-Reward-Modeling/blob/main/bradley-terry-rm/llama3_rm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a65f51-f07f-4107-a403-73db38cec2b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/easymaker/data/dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 환경 변수 설정\n",
    "os.environ['HF_DATASETS_CACHE'] = '/root/easymaker/data/dataset'\n",
    "\n",
    "# 환경 변수가 잘 설정되었는지 확인\n",
    "print(os.environ['HF_DATASETS_CACHE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77668851-2323-4491-b0ae-5e8a16846b41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScriptArguments(local_rank=-1, deepspeed=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, gradient_accumulation_steps=64, learning_rate=2e-06, weight_decay=0.001, model_name='./data/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct', bf16=True, num_train_epochs=1, train_set_path='hendrydong/preference_700K', eval_set_path='hendrydong/preference_700K', output_path='./data/models/llama3_rm_test', gradient_checkpointing=True, optim='paged_adamw_32bit', lr_scheduler_type='cosine', max_length=8192, save_every_steps=999999, eval_every_steps=100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################\n",
    "# This script is modified from the TRL package https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama/scripts/reward_modeling.py\n",
    "# This script is designed for the reward modeling with Mistral model which should be handled carefully because it does not have an official pad token\n",
    "# If you have any question, feel free to send me an email via wx13@illinois.edu\n",
    "########################\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "# import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "# from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.utils import PaddingStrategy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define and parse arguments.\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    \"\"\"\n",
    "    These arguments vary depending on how many GPUs you have, what their capacity and features are, and what size model you want to train.\n",
    "    \"\"\"\n",
    "    local_rank: Optional[int] = field(\n",
    "        default=-1, metadata={\"help\": \"Used for multi-gpu\"})\n",
    "\n",
    "    deepspeed: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Path to deepspeed config if using deepspeed. You may need this if the model that you want to train doesn't fit on a single GPU.\"},\n",
    "    )\n",
    "    \n",
    "    per_device_train_batch_size: Optional[int] = field(default=1)\n",
    "    per_device_eval_batch_size: Optional[int] = field(default=1)\n",
    "    \n",
    "    # for 8 GPU, the global batch size is 512\n",
    "    gradient_accumulation_steps: Optional[int] = field(default=64)\n",
    "    learning_rate: Optional[float] = field(default=2e-6)\n",
    "    weight_decay: Optional[float] = field(default=0.001)\n",
    "    \n",
    "    model_name: Optional[str] = field(\n",
    "        default=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        metadata={\"help\": \"The model that you want to train from the Hugging Face hub. E.g. gpt2, gpt2-xl, bert, etc.\"},\n",
    "    )\n",
    "    bf16: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"This essentially cuts the training time in half if you want to sacrifice a little precision and have a supported GPU.\"},\n",
    "    )\n",
    "    num_train_epochs: Optional[int] = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"The number of training epochs for the reward model.\"},\n",
    "    )\n",
    "    train_set_path: Optional[str] = field(\n",
    "        default=\"hendrydong/preference_700K\",\n",
    "        metadata={\"help\": \"The dir of the subset of the training data to use\"},\n",
    "    )\n",
    "    eval_set_path: Optional[str] = field(\n",
    "        default=\"hendrydong/preference_700K\",\n",
    "        metadata={\"help\": \"The dir of the subset of the eval data to use\"},\n",
    "    )\n",
    "    output_path: Optional[str] = field(\n",
    "        default=\"./models/llama3_rm_test\",\n",
    "        metadata={\"help\": \"The dir for output model\"},\n",
    "    )\n",
    "    gradient_checkpointing: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Enables gradient checkpointing.\"},\n",
    "    )\n",
    "    optim: Optional[str] = field(\n",
    "        # default=\"adamw_hf\",\n",
    "        default=\"paged_adamw_32bit\",\n",
    "        # default=\"adamw_torch_fused\",\n",
    "        metadata={\"help\": \"The optimizer to use.\"},\n",
    "    )\n",
    "    lr_scheduler_type: Optional[str] = field(\n",
    "        default=\"cosine\",\n",
    "        metadata={\"help\": \"The lr scheduler\"},\n",
    "    )\n",
    "    \n",
    "    max_length: Optional[int] = field(default=4096)\n",
    "\n",
    "    save_every_steps: Optional[int] = field(\n",
    "        default=999999,\n",
    "        metadata={\"help\": \"Save the model every x steps\"},\n",
    "    )\n",
    "    eval_every_steps: Optional[int] = field(\n",
    "        default=999999,\n",
    "        metadata={\"help\": \"Eval the model every x steps\"},\n",
    "    )\n",
    "\n",
    "\n",
    "parser = HfArgumentParser(ScriptArguments)\n",
    "script_args = parser.parse_args_into_dataclasses(args=[\n",
    "    '--model_name','./data/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct',\n",
    "    '--train_set_path', 'hendrydong/preference_700K',\n",
    "    '--max_length', '8192',\n",
    "    '--output_path','./data/models/llama3_rm_test',\n",
    "    '--eval_every_steps','100'\n",
    "])[0]\n",
    "\n",
    "script_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c73327d-17cd-4092-82c9-122762deb4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ecc495c540472dbcb3da3336bedd44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/548 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668c566754dd4463bff718b701e3d99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/216M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c481e74c26843079293170b53beb85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/217M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de4224b21ed44ce9371772399e4a6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/218M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf9a1250db94f5fa870585b29b8adc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/218M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa62643a6ea24783a907cf4faa796d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1032a0eb4c24d9585bd9779e96e70fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/322M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e1ffd9ec884d27bf2ed5c667647f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/700000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dfc179203c240618749efaa37d266f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/700000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:  700000  Eval set:  500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the value-head model and tokenizer.\n",
    "tokenizer_name = script_args.model_name\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast = False)\n",
    "\n",
    "# Adjusted according to the base model\n",
    "# Need to do this for the models that don't have an official pad token.\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "print(tokenizer.padding_side)\n",
    "tokenizer.truncation_side = \"left\"\n",
    "tokenizer.model_max_length = script_args.max_length\n",
    "# tokenizer.padding_side = \"right\"\n",
    "\n",
    "\n",
    "\n",
    "# Get the dataset\n",
    "train_path = script_args.train_set_path\n",
    "eval_path = script_args.eval_set_path\n",
    "output_name = script_args.output_path\n",
    "\n",
    "\n",
    "def build_dataset(tokenizer, train_path, eval_path):\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample['positive'] = tokenizer.apply_chat_template(sample['chosen'], tokenize=False, add_generation_prompt=False).replace(tokenizer.bos_token, \"\")\n",
    "        sample['negative'] = tokenizer.apply_chat_template(sample['rejected'], tokenize=False, add_generation_prompt=False).replace(tokenizer.bos_token, \"\")\n",
    "        \n",
    "        tokenized_pos = tokenizer(sample['positive'], truncation=True)\n",
    "        tokenized_neg = tokenizer(sample['negative'], truncation=True)\n",
    "        \n",
    "        sample[\"input_ids_j\"] = tokenized_pos[\"input_ids\"]\n",
    "        sample[\"attention_mask_j\"] = tokenized_pos[\"attention_mask\"]\n",
    "        sample[\"input_ids_k\"] = tokenized_neg[\"input_ids\"]\n",
    "        sample[\"attention_mask_k\"] = tokenized_neg[\"attention_mask\"]\n",
    "        \n",
    "        return sample\n",
    "\n",
    "    ds = load_dataset(train_path, split=\"train\").shuffle(seed=42) # 로드 \n",
    "    \n",
    "    # df = pd.read_csv(train_path)  # CSV 파일 경로\n",
    "    # ds = Dataset.from_pandas(df)  # pandas DataFrame을 Dataset으로 변환\n",
    "    # ds = ds.shuffle(seed=42)  # 데이터 셔플\n",
    "    \n",
    "    # ds = ds.select(range(2000))\n",
    "    ds = ds.map(tokenize, num_proc=8)\n",
    "\n",
    "    eval_dataset = None\n",
    "\n",
    "    train_dataset = ds\n",
    "    #eval_dataset = load_dataset(eval_path, split=\"train\").shuffle(seed=42).select(range(500))\n",
    "    eval_dataset = ds.select(range(500))\n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "\n",
    "train_dataset, eval_dataset = build_dataset(tokenizer, train_path, eval_path)\n",
    "print(\"Training set: \", len(train_dataset), \" Eval set: \", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "215aacd4-811e-4759-9e60-a78bbec5fa2b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rejected': [{'content': 'Solve the following math problem step-by-step.\\nSimplify your answer as much as possible. Present your final answer as \\\\boxed{Your Answer}.\\nthe perimeters of 2 squares are 60 cm and 48 cm . find the perimeter of a third square whose area is equal to the difference of the areas of the two squares ?',\n",
       "   'role': 'user'},\n",
       "  {'content': \"Step 1: First, we need to calculate the area of the two squares. The area of a square is calculated by squaring its side length.\\n\\n\\n\\nAnswer:\\n\\\\boxed{Let's start.\\n\\n1. Area of the first square: side1 = √(perimeter1/4) = √(60/4) = √15.\\n2. Area of the second square: side2 = √(perimeter2/4) = √(48/4) = √12 = √(4*3) = 2√3.\\n3. Difference in their areas: area_diff = side1^2 - side2^2 = 15 - 12 = 3.\\n4. Side length of the new square: side3 = √(area_diff) = √3.\\n5. Perimeter of the new square: perimeter3 = 4*side3 = 4√3.\\n\\nThe answer is <b><4√3 cm</b>.}\",\n",
       "   'role': 'assistant'}],\n",
       " 'rejected_score': None,\n",
       " 'chosen_score': None,\n",
       " 'chosen': [{'content': 'Solve the following math problem step-by-step.\\nSimplify your answer as much as possible. Present your final answer as \\\\boxed{Your Answer}.\\nthe perimeters of 2 squares are 60 cm and 48 cm . find the perimeter of a third square whose area is equal to the difference of the areas of the two squares ?',\n",
       "   'role': 'user'},\n",
       "  {'content': \"Step 1: First, let's calculate the side length of the first square. perimeter = 4 * side_length, so side_length = perimeter / 4 = 60 / 4 = 15 cm.\\nStep 2: Next, calculate the side length of the second square. side_length_2 = 48 / 4 = 12 cm.\\nStep 3: Now, calculate the area of the first square. area_1 = side_length * side_length = 15 * 15 = 225 sq. cm.\\nStep 4: Calculate the area of the second square. area_2 = side_length_2 * side_length_2 = 12 * 12 = 144 sq. cm.\\nStep 5: Find the difference between the areas of the two squares. area_diff = area_1 - area_2 = 225 - 144 = 81 sq. cm.\\nStep 6: Calculate the side length of the third square. side_length_3 = sqrt(area_diff) = sqrt(81) = 9 cm.\\nStep 7: Finally, calculate the perimeter of the third square. perimeter_3 = 4 * side_length_3 = 4 * 9 = 36 cm.\\n\\nAnswer:\\n\\\\boxed{36}\",\n",
       "   'role': 'assistant'}],\n",
       " 'positive': \"<|start_header_id|>user<|end_header_id|>\\n\\nSolve the following math problem step-by-step.\\nSimplify your answer as much as possible. Present your final answer as \\\\boxed{Your Answer}.\\nthe perimeters of 2 squares are 60 cm and 48 cm . find the perimeter of a third square whose area is equal to the difference of the areas of the two squares ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nStep 1: First, let's calculate the side length of the first square. perimeter = 4 * side_length, so side_length = perimeter / 4 = 60 / 4 = 15 cm.\\nStep 2: Next, calculate the side length of the second square. side_length_2 = 48 / 4 = 12 cm.\\nStep 3: Now, calculate the area of the first square. area_1 = side_length * side_length = 15 * 15 = 225 sq. cm.\\nStep 4: Calculate the area of the second square. area_2 = side_length_2 * side_length_2 = 12 * 12 = 144 sq. cm.\\nStep 5: Find the difference between the areas of the two squares. area_diff = area_1 - area_2 = 225 - 144 = 81 sq. cm.\\nStep 6: Calculate the side length of the third square. side_length_3 = sqrt(area_diff) = sqrt(81) = 9 cm.\\nStep 7: Finally, calculate the perimeter of the third square. perimeter_3 = 4 * side_length_3 = 4 * 9 = 36 cm.\\n\\nAnswer:\\n\\\\boxed{36}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
       " 'negative': \"<|start_header_id|>user<|end_header_id|>\\n\\nSolve the following math problem step-by-step.\\nSimplify your answer as much as possible. Present your final answer as \\\\boxed{Your Answer}.\\nthe perimeters of 2 squares are 60 cm and 48 cm . find the perimeter of a third square whose area is equal to the difference of the areas of the two squares ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nStep 1: First, we need to calculate the area of the two squares. The area of a square is calculated by squaring its side length.\\n\\n\\n\\nAnswer:\\n\\\\boxed{Let's start.\\n\\n1. Area of the first square: side1 = √(perimeter1/4) = √(60/4) = √15.\\n2. Area of the second square: side2 = √(perimeter2/4) = √(48/4) = √12 = √(4*3) = 2√3.\\n3. Difference in their areas: area_diff = side1^2 - side2^2 = 15 - 12 = 3.\\n4. Side length of the new square: side3 = √(area_diff) = √3.\\n5. Perimeter of the new square: perimeter3 = 4*side3 = 4√3.\\n\\nThe answer is <b><4√3 cm</b>.}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
       " 'input_ids_j': [128000,\n",
       "  128006,\n",
       "  882,\n",
       "  128007,\n",
       "  271,\n",
       "  50,\n",
       "  4035,\n",
       "  279,\n",
       "  2768,\n",
       "  7033,\n",
       "  3575,\n",
       "  3094,\n",
       "  14656,\n",
       "  30308,\n",
       "  627,\n",
       "  50,\n",
       "  71306,\n",
       "  701,\n",
       "  4320,\n",
       "  439,\n",
       "  1790,\n",
       "  439,\n",
       "  3284,\n",
       "  13,\n",
       "  27740,\n",
       "  701,\n",
       "  1620,\n",
       "  4320,\n",
       "  439,\n",
       "  1144,\n",
       "  80175,\n",
       "  90,\n",
       "  7927,\n",
       "  22559,\n",
       "  28374,\n",
       "  1820,\n",
       "  824,\n",
       "  55336,\n",
       "  315,\n",
       "  220,\n",
       "  17,\n",
       "  32440,\n",
       "  527,\n",
       "  220,\n",
       "  1399,\n",
       "  10166,\n",
       "  323,\n",
       "  220,\n",
       "  2166,\n",
       "  10166,\n",
       "  662,\n",
       "  1505,\n",
       "  279,\n",
       "  47442,\n",
       "  315,\n",
       "  264,\n",
       "  4948,\n",
       "  9518,\n",
       "  6832,\n",
       "  3158,\n",
       "  374,\n",
       "  6273,\n",
       "  311,\n",
       "  279,\n",
       "  6811,\n",
       "  315,\n",
       "  279,\n",
       "  5789,\n",
       "  315,\n",
       "  279,\n",
       "  1403,\n",
       "  32440,\n",
       "  949,\n",
       "  128009,\n",
       "  128006,\n",
       "  78191,\n",
       "  128007,\n",
       "  271,\n",
       "  8468,\n",
       "  220,\n",
       "  16,\n",
       "  25,\n",
       "  5629,\n",
       "  11,\n",
       "  1095,\n",
       "  596,\n",
       "  11294,\n",
       "  279,\n",
       "  3185,\n",
       "  3160,\n",
       "  315,\n",
       "  279,\n",
       "  1176,\n",
       "  9518,\n",
       "  13,\n",
       "  47442,\n",
       "  284,\n",
       "  220,\n",
       "  19,\n",
       "  353,\n",
       "  3185,\n",
       "  5228,\n",
       "  11,\n",
       "  779,\n",
       "  3185,\n",
       "  5228,\n",
       "  284,\n",
       "  47442,\n",
       "  611,\n",
       "  220,\n",
       "  19,\n",
       "  284,\n",
       "  220,\n",
       "  1399,\n",
       "  611,\n",
       "  220,\n",
       "  19,\n",
       "  284,\n",
       "  220,\n",
       "  868,\n",
       "  10166,\n",
       "  627,\n",
       "  8468,\n",
       "  220,\n",
       "  17,\n",
       "  25,\n",
       "  9479,\n",
       "  11,\n",
       "  11294,\n",
       "  279,\n",
       "  3185,\n",
       "  3160,\n",
       "  315,\n",
       "  279,\n",
       "  2132,\n",
       "  9518,\n",
       "  13,\n",
       "  3185,\n",
       "  5228,\n",
       "  62,\n",
       "  17,\n",
       "  284,\n",
       "  220,\n",
       "  2166,\n",
       "  611,\n",
       "  220,\n",
       "  19,\n",
       "  284,\n",
       "  220,\n",
       "  717,\n",
       "  10166,\n",
       "  627,\n",
       "  8468,\n",
       "  220,\n",
       "  18,\n",
       "  25,\n",
       "  4800,\n",
       "  11,\n",
       "  11294,\n",
       "  279,\n",
       "  3158,\n",
       "  315,\n",
       "  279,\n",
       "  1176,\n",
       "  9518,\n",
       "  13,\n",
       "  3158,\n",
       "  62,\n",
       "  16,\n",
       "  284,\n",
       "  3185,\n",
       "  5228,\n",
       "  353,\n",
       "  3185,\n",
       "  5228,\n",
       "  284,\n",
       "  220,\n",
       "  868,\n",
       "  353,\n",
       "  220,\n",
       "  868,\n",
       "  284,\n",
       "  220,\n",
       "  11057,\n",
       "  18522,\n",
       "  13,\n",
       "  10166,\n",
       "  627,\n",
       "  8468,\n",
       "  220,\n",
       "  19,\n",
       "  25,\n",
       "  21157,\n",
       "  279,\n",
       "  3158,\n",
       "  315,\n",
       "  279,\n",
       "  2132,\n",
       "  9518,\n",
       "  13,\n",
       "  3158,\n",
       "  62,\n",
       "  17,\n",
       "  284,\n",
       "  3185,\n",
       "  5228,\n",
       "  62,\n",
       "  17,\n",
       "  353,\n",
       "  3185,\n",
       "  5228,\n",
       "  62,\n",
       "  17,\n",
       "  284,\n",
       "  220,\n",
       "  717,\n",
       "  353,\n",
       "  220,\n",
       "  717,\n",
       "  284,\n",
       "  220,\n",
       "  8929,\n",
       "  18522,\n",
       "  13,\n",
       "  10166,\n",
       "  627,\n",
       "  8468,\n",
       "  220,\n",
       "  20,\n",
       "  25,\n",
       "  7531,\n",
       "  279,\n",
       "  6811,\n",
       "  1990,\n",
       "  279,\n",
       "  5789,\n",
       "  315,\n",
       "  279,\n",
       "  1403,\n",
       "  32440,\n",
       "  13,\n",
       "  3158,\n",
       "  16229,\n",
       "  284,\n",
       "  3158,\n",
       "  62,\n",
       "  16,\n",
       "  482,\n",
       "  3158,\n",
       "  62,\n",
       "  17,\n",
       "  284,\n",
       "  220,\n",
       "  11057,\n",
       "  482,\n",
       "  220,\n",
       "  8929,\n",
       "  284,\n",
       "  220,\n",
       "  5932,\n",
       "  18522,\n",
       "  13,\n",
       "  10166,\n",
       "  627,\n",
       "  8468,\n",
       "  220,\n",
       "  21,\n",
       "  25,\n",
       "  21157,\n",
       "  279,\n",
       "  3185,\n",
       "  3160,\n",
       "  315,\n",
       "  279,\n",
       "  4948,\n",
       "  9518,\n",
       "  13,\n",
       "  3185,\n",
       "  5228,\n",
       "  62,\n",
       "  18,\n",
       "  284,\n",
       "  18430,\n",
       "  56802,\n",
       "  16229,\n",
       "  8,\n",
       "  284,\n",
       "  18430,\n",
       "  7,\n",
       "  5932,\n",
       "  8,\n",
       "  284,\n",
       "  220,\n",
       "  24,\n",
       "  10166,\n",
       "  627,\n",
       "  8468,\n",
       "  220,\n",
       "  22,\n",
       "  25,\n",
       "  17830,\n",
       "  11,\n",
       "  11294,\n",
       "  279,\n",
       "  47442,\n",
       "  315,\n",
       "  279,\n",
       "  4948,\n",
       "  9518,\n",
       "  13,\n",
       "  47442,\n",
       "  62,\n",
       "  18,\n",
       "  284,\n",
       "  220,\n",
       "  19,\n",
       "  353,\n",
       "  3185,\n",
       "  5228,\n",
       "  62,\n",
       "  18,\n",
       "  284,\n",
       "  220,\n",
       "  19,\n",
       "  353,\n",
       "  220,\n",
       "  24,\n",
       "  284,\n",
       "  220,\n",
       "  1927,\n",
       "  10166,\n",
       "  382,\n",
       "  16533,\n",
       "  512,\n",
       "  59,\n",
       "  80175,\n",
       "  90,\n",
       "  1927,\n",
       "  92,\n",
       "  128009,\n",
       "  128006,\n",
       "  78191,\n",
       "  128007,\n",
       "  271],\n",
       " 'attention_mask_j': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'input_ids_k': [128000,\n",
       "  128006,\n",
       "  882,\n",
       "  128007,\n",
       "  271,\n",
       "  50,\n",
       "  4035,\n",
       "  279,\n",
       "  2768,\n",
       "  7033,\n",
       "  3575,\n",
       "  3094,\n",
       "  14656,\n",
       "  30308,\n",
       "  627,\n",
       "  50,\n",
       "  71306,\n",
       "  701,\n",
       "  4320,\n",
       "  439,\n",
       "  1790,\n",
       "  439,\n",
       "  3284,\n",
       "  13,\n",
       "  27740,\n",
       "  701,\n",
       "  1620,\n",
       "  4320,\n",
       "  439,\n",
       "  1144,\n",
       "  80175,\n",
       "  90,\n",
       "  7927,\n",
       "  22559,\n",
       "  28374,\n",
       "  1820,\n",
       "  824,\n",
       "  55336,\n",
       "  315,\n",
       "  220,\n",
       "  17,\n",
       "  32440,\n",
       "  527,\n",
       "  220,\n",
       "  1399,\n",
       "  10166,\n",
       "  323,\n",
       "  220,\n",
       "  2166,\n",
       "  10166,\n",
       "  662,\n",
       "  1505,\n",
       "  279,\n",
       "  47442,\n",
       "  315,\n",
       "  264,\n",
       "  4948,\n",
       "  9518,\n",
       "  6832,\n",
       "  3158,\n",
       "  374,\n",
       "  6273,\n",
       "  311,\n",
       "  279,\n",
       "  6811,\n",
       "  315,\n",
       "  279,\n",
       "  5789,\n",
       "  315,\n",
       "  279,\n",
       "  1403,\n",
       "  32440,\n",
       "  949,\n",
       "  128009,\n",
       "  128006,\n",
       "  78191,\n",
       "  128007,\n",
       "  271,\n",
       "  8468,\n",
       "  220,\n",
       "  16,\n",
       "  25,\n",
       "  5629,\n",
       "  11,\n",
       "  584,\n",
       "  1205,\n",
       "  311,\n",
       "  11294,\n",
       "  279,\n",
       "  3158,\n",
       "  315,\n",
       "  279,\n",
       "  1403,\n",
       "  32440,\n",
       "  13,\n",
       "  578,\n",
       "  3158,\n",
       "  315,\n",
       "  264,\n",
       "  9518,\n",
       "  374,\n",
       "  16997,\n",
       "  555,\n",
       "  8330,\n",
       "  3329,\n",
       "  1202,\n",
       "  3185,\n",
       "  3160,\n",
       "  2055,\n",
       "  16533,\n",
       "  512,\n",
       "  59,\n",
       "  80175,\n",
       "  90,\n",
       "  10267,\n",
       "  596,\n",
       "  1212,\n",
       "  382,\n",
       "  16,\n",
       "  13,\n",
       "  12299,\n",
       "  315,\n",
       "  279,\n",
       "  1176,\n",
       "  9518,\n",
       "  25,\n",
       "  3185,\n",
       "  16,\n",
       "  284,\n",
       "  122371,\n",
       "  7,\n",
       "  716,\n",
       "  26402,\n",
       "  16,\n",
       "  14,\n",
       "  19,\n",
       "  8,\n",
       "  284,\n",
       "  122371,\n",
       "  7,\n",
       "  1399,\n",
       "  14,\n",
       "  19,\n",
       "  8,\n",
       "  284,\n",
       "  122371,\n",
       "  868,\n",
       "  627,\n",
       "  17,\n",
       "  13,\n",
       "  12299,\n",
       "  315,\n",
       "  279,\n",
       "  2132,\n",
       "  9518,\n",
       "  25,\n",
       "  3185,\n",
       "  17,\n",
       "  284,\n",
       "  122371,\n",
       "  7,\n",
       "  716,\n",
       "  26402,\n",
       "  17,\n",
       "  14,\n",
       "  19,\n",
       "  8,\n",
       "  284,\n",
       "  122371,\n",
       "  7,\n",
       "  2166,\n",
       "  14,\n",
       "  19,\n",
       "  8,\n",
       "  284,\n",
       "  122371,\n",
       "  717,\n",
       "  284,\n",
       "  122371,\n",
       "  7,\n",
       "  19,\n",
       "  9,\n",
       "  18,\n",
       "  8,\n",
       "  284,\n",
       "  220,\n",
       "  17,\n",
       "  110682,\n",
       "  18,\n",
       "  627,\n",
       "  18,\n",
       "  13,\n",
       "  56180,\n",
       "  304,\n",
       "  872,\n",
       "  5789,\n",
       "  25,\n",
       "  3158,\n",
       "  16229,\n",
       "  284,\n",
       "  3185,\n",
       "  16,\n",
       "  61,\n",
       "  17,\n",
       "  482,\n",
       "  3185,\n",
       "  17,\n",
       "  61,\n",
       "  17,\n",
       "  284,\n",
       "  220,\n",
       "  868,\n",
       "  482,\n",
       "  220,\n",
       "  717,\n",
       "  284,\n",
       "  220,\n",
       "  18,\n",
       "  627,\n",
       "  19,\n",
       "  13,\n",
       "  17072,\n",
       "  3160,\n",
       "  315,\n",
       "  279,\n",
       "  502,\n",
       "  9518,\n",
       "  25,\n",
       "  3185,\n",
       "  18,\n",
       "  284,\n",
       "  122371,\n",
       "  7,\n",
       "  4903,\n",
       "  16229,\n",
       "  8,\n",
       "  284,\n",
       "  122371,\n",
       "  18,\n",
       "  627,\n",
       "  20,\n",
       "  13,\n",
       "  3700,\n",
       "  26402,\n",
       "  315,\n",
       "  279,\n",
       "  502,\n",
       "  9518,\n",
       "  25,\n",
       "  47442,\n",
       "  18,\n",
       "  284,\n",
       "  220,\n",
       "  19,\n",
       "  9,\n",
       "  3002,\n",
       "  18,\n",
       "  284,\n",
       "  220,\n",
       "  19,\n",
       "  110682,\n",
       "  18,\n",
       "  382,\n",
       "  791,\n",
       "  4320,\n",
       "  374,\n",
       "  366,\n",
       "  65,\n",
       "  1822,\n",
       "  19,\n",
       "  110682,\n",
       "  18,\n",
       "  10166,\n",
       "  524,\n",
       "  65,\n",
       "  14611,\n",
       "  92,\n",
       "  128009,\n",
       "  128006,\n",
       "  78191,\n",
       "  128007,\n",
       "  271],\n",
       " 'attention_mask_k': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3f4910b-55c1-49a9-81d4-0e17df615d20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/easymaker/custom-conda-envs/ai_llm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdad505aa8664d9e8109ab07a82f1449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at ./data/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define the trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_name,\n",
    "    learning_rate=script_args.learning_rate,\n",
    "    per_device_train_batch_size=script_args.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n",
    "    num_train_epochs=script_args.num_train_epochs,\n",
    "    weight_decay=script_args.weight_decay,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=script_args.eval_every_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=script_args.save_every_steps,\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "    gradient_checkpointing=script_args.gradient_checkpointing,\n",
    "    deepspeed=script_args.deepspeed,\n",
    "    local_rank=script_args.local_rank,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[],\n",
    "    bf16=script_args.bf16,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    optim=script_args.optim,\n",
    "    lr_scheduler_type=script_args.lr_scheduler_type,\n",
    "    warmup_ratio=0.03,\n",
    "    # report_to='wandb'\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(script_args.model_name, num_labels=1, torch_dtype=torch.bfloat16, use_flash_attention_2=True,)\n",
    "\n",
    "model.config.use_cache = not script_args.gradient_checkpointing\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "num_proc = 24  # Can adjust to be higher if you have more processors.\n",
    "original_columns = train_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6323c0df-d734-4825-a8bb-aaa0a7ad1895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# We need to define a special data collator that batches the data in our j vs k format.\n",
    "@dataclass\n",
    "class RewardDataCollatorWithPadding:\n",
    "    tokenizer: AutoTokenizer\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        merged_features = []\n",
    "\n",
    "        for feature in features:\n",
    "            merged_features.append(\n",
    "                {\n",
    "                    \"input_ids\": feature[\"input_ids_j\"],\n",
    "                    \"attention_mask\": feature[\"attention_mask_j\"],\n",
    "                }\n",
    "            )\n",
    "            merged_features.append(\n",
    "                {\n",
    "                    \"input_ids\": feature[\"input_ids_k\"],\n",
    "                    \"attention_mask\": feature[\"attention_mask_k\"],\n",
    "                }\n",
    "            )\n",
    "        batch = self.tokenizer.pad(\n",
    "            merged_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        batch = {\n",
    "            \"input_ids\": batch[\"input_ids\"],\n",
    "            \"attention_mask\": batch[\"attention_mask\"],\n",
    "            \"return_loss\": True,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "\n",
    "# Define the trainer\n",
    "def compute_metrics(eval_pred):\n",
    "    result = {}\n",
    "    pos_predictions_scores = eval_pred.predictions[0]\n",
    "    neg_predictions_scores = eval_pred.predictions[1]\n",
    "    \n",
    "    # We assume that the first sample is preferred by default in groundtruth\n",
    "    result['accuracy'] = np.sum(pos_predictions_scores > neg_predictions_scores) / len(pos_predictions_scores)\n",
    "    return result\n",
    "\n",
    "\n",
    "class RewardTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        rewards = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])[0]\n",
    "        bsz = rewards.size(0)\n",
    "        jidx = torch.arange(0, bsz, 2)\n",
    "        kidx = jidx + 1\n",
    "        rewards_j = rewards[jidx]\n",
    "        rewards_k = rewards[kidx]\n",
    "        loss = -nn.functional.logsigmoid(rewards_j - rewards_k).mean()\n",
    "        if return_outputs:\n",
    "            return loss, {\"rewards_j\": rewards_j, \"rewards_k\": rewards_k}\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf8a340-fdd7-426d-8e81-b51c0c30411d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/root/easymaker/custom-conda-envs/ai_llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/root/easymaker/custom-conda-envs/ai_llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='201' max='10937' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  201/10937 1:00:17 < 54:12:21, 0.06 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.897000</td>\n",
       "      <td>0.828445</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>0.673492</td>\n",
       "      <td>0.636000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model, woohoo.\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=RewardDataCollatorWithPadding(tokenizer=tokenizer, max_length=script_args.max_length),\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "print(\"Saving last checkpoint of the model\")\n",
    "trainer.save_model(output_name + \"/last_checkpoint\")\n",
    "tokenizer.save_pretrained(output_name + \"/last_checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaf5f90-6adf-42c4-b3c5-e322cfd4cc93",
   "metadata": {},
   "source": [
    "추론 속도 500개 30초정도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0ef186-84f9-4bd5-a129-044245600c42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_llm",
   "language": "python",
   "name": "ai_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
